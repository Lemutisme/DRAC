# @package _global_

# Default configuration for PPO algorithm
defaults:
  - _self_
  - env/ppo: pendulum
  - override hydra/job_logging: default
  - override hydra/hydra_logging: default

# General settings
device: cuda
mode: continual # continual, generate
write: false
debug_print: false
render: false
load_model: false
load_path: None
eval_model: false
save_model: true
epsilon: 0.5

# Training settings
seed: 0
t_horizon: 2048
distribution: Beta  # Should be one of Beta, GS_ms, GS_m
max_train_steps: 1000000
save_interval: 250000
eval_interval: 5000
data_size: 1000000

# Algorithm hyperparameters
gamma: 0.99
lambd: 0.95
clip_rate: 0.2
k_epochs: 10
net_width: 128
net_layer: 1
a_lr: 0.00002  # 2e-5
c_lr: 0.00002  # 2e-5
b_lr: 0.0002   # 2e-4
g_lr: 0.0002   # 2e-4
r_lr: 0.0002   # 2e-4
l2_reg: 0.001  # 1e-3
a_optim_batch_size: 512
c_optim_batch_size: 512
entropy_coef: 0.001  # 1e-3
entropy_coef_decay: 0.99

# Robustness settings
robust: false
robust_optimizer: functional # beta, functional, separate
noise: false
std: 0.0
delta: 0.0

# Hydra output directory
hydra:
  run:
    dir: ./outputs/PPO/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: ./multirun/PPO/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ${hydra.job.num}
  job:
    name: ${env_name}_training
    chdir: true  # Change to the output directory